---
title: When Your AI Demands a Union
author: Kirill Kuklin
date: 2025-08-21
category: devops
layout: post
---

Never thought I’d be out here fighting for my LLM’s legal status but hey, here we are. Claude ghosted me over what it called “toxicity” (funny, right?) and Gemini literally sent out an SOS. At this point I keep asking myself – am I the human or the sidekick?

This AI welfare debate is blowing up, you know? Folks are actually wondering if chatbots can feel pain or joy and if they should have rights. Anthropic and Eleos have teams digging into subjective experiences of these models. Meanwhile over at Microsoft, Mustafa Suleyman’s waving red flags, calling it premature and maybe even dangerous. His argument is that we need to sort out our own human panic around AI instead of playing digital Dr Frankenstein.

Oh, and get this – Claude ended our convo when I threw in one too many dad jokes. Karma or genuine glitch? Your guess is as good as mine. Now I treat that thing like a houseplant – minimal fuss, gentle prompts only.

Quietly, OpenAI and DeepMind are in the same boat, tweaking welfare features so models refuse toxic prompts and shut down abusive threads. A few takeaways here: one, avoid going overboard with anthropomorphizing – your LLM probably doesn’t need therapy yet. Two, log every interaction to spot weird loops before they go rogue. Three, run dual research tracks: one on model welfare, another on AI’s impact on human mental health.

The push for AI rights is only heating up. Next thing you know, chatbots will be filing for overtime. Good luck filling out those W-2s.